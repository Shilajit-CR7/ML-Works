{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shilajit-CR7/ML-Works/blob/main/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df9303d",
      "metadata": {
        "id": "7df9303d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e40061d",
      "metadata": {
        "id": "6e40061d"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d91a6ec4",
      "metadata": {
        "id": "d91a6ec4",
        "outputId": "5ba49ffb-bb56-4b08-abf8-7396935a8fc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to C:\\Users\\Shilajit-\n",
            "[nltk_data]     CR7\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b3e60c",
      "metadata": {
        "id": "52b3e60c",
        "outputId": "b85e203b-3a51-48d9-c974-7526bd405187"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Shilajit-\n",
            "[nltk_data]     CR7\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a325831",
      "metadata": {
        "id": "6a325831",
        "outputId": "e48a0898-a416-4086-850c-ab35129e3615"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "brown.categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da868c58",
      "metadata": {
        "id": "da868c58",
        "outputId": "cbf2c9d5-1a26-4e7c-9bff-86965e5930dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['They', 'neither', 'liked', 'nor', 'disliked', 'the', 'Old', 'Man', '.'], ['To', 'them', 'he', 'could', 'have', 'been', 'the', 'broken', 'bell', 'in', 'the', 'church', 'tower', 'which', 'rang', 'before', 'and', 'after', 'Mass', ',', 'and', 'at', 'noon', ',', 'and', 'at', 'six', 'each', 'evening', '--', 'its', 'tone', ',', 'repetitive', ',', 'monotonous', ',', 'never', 'breaking', 'the', 'boredom', 'of', 'the', 'streets', '.'], ...]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = brown.sents(categories='romance')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2503a6c",
      "metadata": {
        "id": "e2503a6c",
        "outputId": "43be525f-a7d8-4891-fead-4d1e5dcc8206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'They neither liked nor disliked the Old Man .'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join(data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d99463",
      "metadata": {
        "id": "55d99463"
      },
      "source": [
        "# Bag of Words Pipeline\n",
        "1. Get the data\n",
        "2. Tokenisation and stopword removal\n",
        "3. Stemming\n",
        "4. Builiding Vocab\n",
        "5. Vectorization \n",
        "6. Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98112b39",
      "metadata": {
        "id": "98112b39"
      },
      "source": [
        "Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc57cbb",
      "metadata": {
        "id": "9cc57cbb"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb3b96c",
      "metadata": {
        "id": "6bb3b96c"
      },
      "outputs": [],
      "source": [
        "document = \"It was a very pleasant day. The weather was cool and there was light showers. I went to the market to buy some fruits.\"\n",
        "sentence = \"Send all the 50 documents related to chapters 1,2,3 at prateek@cb.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19db05e4",
      "metadata": {
        "id": "19db05e4",
        "outputId": "5c456f10-660e-4d1f-9486-d06c0ebc8804"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['It was a very pleasant day.',\n",
              " 'The weather was cool and there was light showers.',\n",
              " 'I went to the market to buy some fruits.']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents = sent_tokenize(document)\n",
        "# Breaks the document sentence by sentence\n",
        "sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb60228",
      "metadata": {
        "id": "cfb60228",
        "outputId": "50ad2d58-82c6-41e2-b269-70444a6a6a78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Send',\n",
              " 'all',\n",
              " 'the',\n",
              " '50',\n",
              " 'documents',\n",
              " 'related',\n",
              " 'to',\n",
              " 'chapters',\n",
              " '1,2,3',\n",
              " 'at',\n",
              " 'prateek',\n",
              " '@',\n",
              " 'cb.com']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = word_tokenize(sentence)\n",
        "# Breaks the document word by word \n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b18b45",
      "metadata": {
        "id": "14b18b45",
        "outputId": "ecfff6ac-e84c-425b-b40d-803df9a15417"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Shilajit-\n",
            "[nltk_data]     CR7\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22110a3c",
      "metadata": {
        "id": "22110a3c"
      },
      "outputs": [],
      "source": [
        "sw = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003787ba",
      "metadata": {
        "id": "003787ba",
        "outputId": "7b4cd696-d8dc-494b-e2c1-9ac5aa2bef9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9bd4270",
      "metadata": {
        "id": "c9bd4270"
      },
      "outputs": [],
      "source": [
        "def remove_sw(text,sw):\n",
        "    useful_words = [w for w in text if w not in sw]\n",
        "    return useful_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e37865c",
      "metadata": {
        "id": "2e37865c",
        "outputId": "92732da9-2b98-40ee-a0d1-fb831a9a2af9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'that.']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"I was doing that.\".split() # Always give word tokenized input\n",
        "remove_sw(text,sw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "443696cb",
      "metadata": {
        "id": "443696cb"
      },
      "source": [
        "Tokenization using Regular Expression "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af27122",
      "metadata": {
        "id": "4af27122"
      },
      "outputs": [],
      "source": [
        "sentence = \"Send all the 50 documents related to chapters 1,2,3 at prateek@cb.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3506d0",
      "metadata": {
        "id": "ac3506d0",
        "outputId": "8ddfa8b1-3777-4c52-ba1a-f308a22337c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Send',\n",
              " 'all',\n",
              " 'the',\n",
              " '50',\n",
              " 'documents',\n",
              " 'related',\n",
              " 'to',\n",
              " 'chapters',\n",
              " '1,2,3',\n",
              " 'at',\n",
              " 'prateek@cb.com']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence.split()\n",
        "# 1,2,3 or 50 is not tokenized. \n",
        "# Even with word_tokenize() we can't do this.\n",
        "# so use regular exp tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99ae7b1a",
      "metadata": {
        "id": "99ae7b1a"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54e3dae0",
      "metadata": {
        "id": "54e3dae0"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer('[a-zA-Z@]+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7712e8b6",
      "metadata": {
        "id": "7712e8b6",
        "outputId": "a9f4c1aa-1154-4d12-c790-5fb9ef3c3f3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Send',\n",
              " 'all',\n",
              " 'the',\n",
              " 'documents',\n",
              " 'related',\n",
              " 'to',\n",
              " 'chapters',\n",
              " 'at',\n",
              " 'prateek@cb',\n",
              " 'com']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "useful_text = tokenizer.tokenize(sentence)\n",
        "useful_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d82241a",
      "metadata": {
        "id": "2d82241a"
      },
      "source": [
        "# Stemming\n",
        "It actually tranforms the word into it's base format (ex: Running ==> Run)\n",
        "Nltk provides 3 types of stemmer => 1. Snowball 2. Porter 3. Lancaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d401be9",
      "metadata": {
        "id": "0d401be9"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a358b2",
      "metadata": {
        "id": "92a358b2"
      },
      "outputs": [],
      "source": [
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d71119",
      "metadata": {
        "id": "66d71119",
        "outputId": "e8325825-fb46-4c8b-9c35-2030fd5eeed6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'run'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ps.stem('Running')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856ca8bd",
      "metadata": {
        "id": "856ca8bd"
      },
      "outputs": [],
      "source": [
        "ss = SnowballStemmer('english')\n",
        "# Here you have to give the language as it supports other languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a137cb6",
      "metadata": {
        "id": "0a137cb6",
        "outputId": "8d673eef-f093-4cf5-957a-5fa41bf28218"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' i was go'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ss.stem(' I was Going')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307477d5",
      "metadata": {
        "id": "307477d5",
        "outputId": "37427062-3ee9-463b-f49d-c1ca53380c55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Shilajit-\n",
            "[nltk_data]     CR7\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shilajit-\n",
            "[nltk_data]     CR7\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wn = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# Either stemming or lemmatiztion needed "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649da665",
      "metadata": {
        "id": "649da665",
        "outputId": "269070d3-568f-46a5-eea1-12d60307f0ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Jumping'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wn.lemmatize('Jumping')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4266621d",
      "metadata": {
        "id": "4266621d"
      },
      "source": [
        "# Vectoriztion\n",
        "- What word is coming how many times \n",
        "- Use sklearn CountVectorizer  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e7e53a",
      "metadata": {
        "id": "b1e7e53a"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    'Real Madrid wins the club world cup.',\n",
        "    'Modi will win the elections.',\n",
        "    'That man won hearts of the people.',\n",
        "    'The movie Razzi is spy thriller'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd52e5ba",
      "metadata": {
        "id": "cd52e5ba",
        "outputId": "f184b930-98e1-409d-b77c-2bc78ed09d23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Real Madrid wins the club world cup.',\n",
              " 'Modi will win the elections.',\n",
              " 'That man won hearts of the people.',\n",
              " 'The movie Razzi is spy thriller']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23719258",
      "metadata": {
        "id": "23719258"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce61e79",
      "metadata": {
        "id": "7ce61e79"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2c0aa0",
      "metadata": {
        "id": "cd2c0aa0",
        "outputId": "60ec8f80-c1c7-4d7f-ec8c-961ea4957c41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<4x22 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 25 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vect_corpus = cv.fit_transform(corpus)\n",
        "vect_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9104f0",
      "metadata": {
        "id": "fc9104f0",
        "outputId": "7d6e0793-5b6a-41f1-9917-d1d736a01379"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0]],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vect_corpus = vect_corpus.toarray()\n",
        "vect_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d8615b",
      "metadata": {
        "id": "72d8615b",
        "outputId": "ae5653eb-821d-45ae-8c8e-4c010c8324a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'real': 12,\n",
              " 'madrid': 5,\n",
              " 'wins': 19,\n",
              " 'the': 15,\n",
              " 'club': 0,\n",
              " 'world': 21,\n",
              " 'cup': 1,\n",
              " 'modi': 7,\n",
              " 'will': 17,\n",
              " 'win': 18,\n",
              " 'elections': 2,\n",
              " 'that': 14,\n",
              " 'man': 6,\n",
              " 'won': 20,\n",
              " 'hearts': 3,\n",
              " 'of': 9,\n",
              " 'people': 10,\n",
              " 'movie': 8,\n",
              " 'razzi': 11,\n",
              " 'is': 4,\n",
              " 'spy': 13,\n",
              " 'thriller': 16}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv.vocabulary_\n",
        "# real : 12th idx\n",
        "# wins : 19th idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c52b12c",
      "metadata": {
        "id": "0c52b12c",
        "outputId": "33989447-bf84-40ca-85d2-79394ef79157"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array(['club', 'cup', 'madrid', 'real', 'the', 'wins', 'world'],\n",
              "       dtype='<U9')]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = cv.inverse_transform(vect_corpus[0].reshape(1, -1))\n",
        "s\n",
        "# From bag of words to sentece (inverse)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c22bc0db",
      "metadata": {
        "id": "c22bc0db"
      },
      "source": [
        "# Vectorization with StopWord Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab70440f",
      "metadata": {
        "id": "ab70440f"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer('[a-zA-Z@]+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c13319",
      "metadata": {
        "id": "65c13319"
      },
      "outputs": [],
      "source": [
        "def remove_sw(text,sw):\n",
        "    useful_words = [w for w in text if w not in sw]\n",
        "    return useful_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa08e56e",
      "metadata": {
        "id": "aa08e56e"
      },
      "outputs": [],
      "source": [
        "def myTokenizer(document):\n",
        "    words = tokenizer.tokenize(document.lower())\n",
        "    words = remove_sw(words,sw)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429e8e6a",
      "metadata": {
        "id": "429e8e6a",
        "outputId": "f6ef9ed9-d716-41e5-8e21-2c4ab5adc237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['function']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "myTokenizer('This is some function')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9a0d3f",
      "metadata": {
        "id": "8b9a0d3f",
        "outputId": "4483520d-1b55-4ed4-89de-47c101c95d87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Real Madrid wins the club world cup.',\n",
              " 'Modi will win the elections.',\n",
              " 'That man won hearts of the people.',\n",
              " 'The movie Razzi is spy thriller']"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4cf8dd",
      "metadata": {
        "id": "4c4cf8dd"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(tokenizer=myTokenizer)\n",
        "vect_corpus = cv.fit_transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33853f97",
      "metadata": {
        "id": "33853f97",
        "outputId": "cbf70e3b-45f2-4cda-8684-5072486736fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0]], dtype=int64)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vect_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff07a9d",
      "metadata": {
        "id": "5ff07a9d",
        "outputId": "0b589eb0-62e5-4821-ed5c-0f0c01e79b88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array(['club', 'cup', 'madrid', 'real', 'wins', 'world'], dtype='<U9'),\n",
              " array(['elections', 'modi', 'win'], dtype='<U9'),\n",
              " array(['hearts', 'man', 'people'], dtype='<U9'),\n",
              " array(['movie', 'razzi', 'spy', 'thriller'], dtype='<U9')]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv.inverse_transform(vect_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b1e216",
      "metadata": {
        "id": "f2b1e216"
      },
      "outputs": [],
      "source": [
        "# Traing data fit _transform. Test data only transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc63e274",
      "metadata": {
        "id": "bc63e274"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26fb763",
      "metadata": {
        "id": "f26fb763"
      },
      "source": [
        "# More Ways to Create Features \n",
        "- Unigram: Every word as a feature\n",
        "- Bigrams\n",
        "- Trigrams\n",
        "- n-grams\n",
        "- TF-IDF Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87487e31",
      "metadata": {
        "id": "87487e31"
      },
      "outputs": [],
      "source": [
        "s1 = ['This is a good movie.']\n",
        "s2 = ['This is not a good movie.']\n",
        "# s1 = good s2 = not good. Take 2 word features\n",
        "# otherwise it will classify both s1 and s2 as good movies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86916b5b",
      "metadata": {
        "id": "86916b5b"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(ngram_range=(2,2))\n",
        "#(2,2) = two word feature "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5726fb19",
      "metadata": {
        "id": "5726fb19",
        "outputId": "3d95d5d0-8e73-46dd-c877-0eb385a67f28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 1]], dtype=int64)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc = [s1[0],s2[0]]\n",
        "cv.fit_transform(doc).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e00323c6",
      "metadata": {
        "id": "e00323c6",
        "outputId": "777801bc-40ae-454c-8941-87cf7c8ebb75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'this is': 4, 'is good': 1, 'good movie': 0, 'is not': 2, 'not good': 3}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv.vocabulary_\n",
        "# 'this is' is one feature (two word feature --> bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd52d0ff",
      "metadata": {
        "id": "bd52d0ff",
        "outputId": "9f99a9d9-a8ae-4810-ee26-338782b34ee8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 1, 0],\n",
              "       [0, 1, 1, 0, 1]], dtype=int64)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv = CountVectorizer(ngram_range=(3,3))\n",
        "doc = [s1[0],s2[0]]\n",
        "cv.fit_transform(doc).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca342c1",
      "metadata": {
        "id": "8ca342c1",
        "outputId": "c875afa1-77ea-478f-e274-bc44205ac661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'this is good': 3,\n",
              " 'is good movie': 0,\n",
              " 'this is not': 4,\n",
              " 'is not good': 1,\n",
              " 'not good movie': 2}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv.vocabulary_\n",
        "# 3 word feature. See n-word vectorizer find optimum n."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7625d9d8",
      "metadata": {
        "id": "7625d9d8"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5bb6e51",
      "metadata": {
        "id": "c5bb6e51"
      },
      "outputs": [],
      "source": [
        "s1 = 'this is good movie.'\n",
        "s2 = 'this was good movie.'\n",
        "s3 = 'this is not good movie.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f2bd40",
      "metadata": {
        "id": "00f2bd40"
      },
      "outputs": [],
      "source": [
        "corpus = [s1,s2,s3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "669666a6",
      "metadata": {
        "id": "669666a6",
        "outputId": "b18bfb81-32a2-4808-8a11-2d113a01c2d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this is good movie.', 'this was good movie.', 'this is not good movie.']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "645c148a",
      "metadata": {
        "id": "645c148a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4161adc3",
      "metadata": {
        "id": "4161adc3"
      },
      "outputs": [],
      "source": [
        "vc  = tfidf.fit_transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d690a2",
      "metadata": {
        "id": "57d690a2",
        "outputId": "49913dcb-50c8-44d0-ef3b-0e048a937037"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd85fa47",
      "metadata": {
        "id": "cd85fa47"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "NLTK.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}